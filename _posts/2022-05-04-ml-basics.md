Even though the theory behind deep learning should be fairly simple, I've been trying to piece it together for me. My understanding may not be entirely acurate yet, but I'm dedicated on updating this post. Any given function can be turned into a deep learning function. These are the very basic steps that make up deep learning.

1. Initialize the weights. Weights or parameters are added to a function to help predict a desired outcome. Weights are random initially and with each iteration, they are fine tuned to predict more accurately.

2. Predict. This is essentially what we want to predict with our function.

3. Based on the prediction and the ground truth of the training data, calculate how good the model is performing (its loss).

4. Calculate the gradient (derivative of the function), which measures for each weight how changing that weight would change the loss.

5. Step. How much we change the weights based on the calculation of the gradient.

In this step, changing the weight by a small amount is ineffective. By calculating the gradients, we can figure out in which direction and by how much we should we should change each weight. The gradients are calculated by figuring out how x in a function changes related to the weight of that function. This is the derivative of a function, which is another function in itself calculating the change rather than the value. Our function will have a lot of weights and the derivative for each weight is calculated. Once the derivative is calculated, we get the value of the derivative function for a particular argument.

In deep learning jargon, this is the process of backpropagation, meaning that based on the gradient descent, the updated weights are propagated back throughout all the layers of the neural network.

6. Stop or go back to step 2 and repeat. Stops if the elapsed iterations that we defined have completed, or if we defined a target for a desired loss or accuracy and this was achieved. If not we repeat the previous steps until the end conditions are met.


